# ğŸ” Dynamic Anomaly Detection System

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A production-ready machine learning system for **real-time anomaly detection** on transaction data with **dynamic feature adaptation**. This system automatically handles scenarios where new features are added, features are missing, or feature sets evolve over timeâ€”all without requiring model retraining.

## ğŸŒŸ Key Features

### ğŸš€ Dynamic Feature Adaptation
- **Automatic Feature Handling**: Seamlessly adapts when new features are introduced (e.g., 10 â†’ 15 features)
- **Missing Feature Tolerance**: Gracefully handles missing features using intelligent default values
- **Backward Compatibility**: Maintains compatibility with existing features while supporting new ones
- **Zero-Downtime Updates**: No model retraining required when features change

### âš¡ Production-Ready
- **FastAPI REST API**: High-performance, async API for real-time inference
- **Docker Support**: Fully containerized for easy deployment
- **Scalable Architecture**: Stateless design enables horizontal scaling
- **Health Checks**: Built-in monitoring and health endpoints
- **Comprehensive Testing**: Unit tests and example usage included

### ğŸ¯ Advanced ML Capabilities
- **Isolation Forest Algorithm**: Robust unsupervised anomaly detection
- **Feature Statistics Tracking**: Maintains statistics for intelligent default values
- **Incremental Learning Support**: Update model with new data
- **Model Persistence**: Save and load models for production use

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage Examples](#usage-examples)
- [API Documentation](#api-documentation)
- [How Dynamic Features Work](#how-dynamic-features-work)
- [Deployment](#deployment)
- [Performance](#performance)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Overview

Traditional anomaly detection models break when the feature set changes. This system solves that problem by:

1. **Learning initial features** during training
2. **Automatically detecting new features** during inference
3. **Using intelligent defaults** for missing features
4. **Maintaining feature statistics** for robust handling

### Use Cases

- ğŸ’³ **Financial Fraud Detection**: Detect fraudulent transactions as new features are added
- ğŸ›’ **E-commerce Anomaly Detection**: Identify unusual purchase patterns with evolving attributes
- ğŸ”Œ **IoT Sensor Monitoring**: Handle sensor data with variable attributes
- ğŸ“Š **Log Analysis**: Detect anomalies in system logs with changing formats
- ğŸ”’ **Network Security**: Analyze network traffic with dynamic feature sets

## ğŸ—ï¸ Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI REST API                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  /train  â”‚  â”‚ /predict â”‚  â”‚ /predict â”‚  â”‚ /update  â”‚   â”‚
â”‚  â”‚          â”‚  â”‚          â”‚  â”‚  /batch  â”‚  â”‚          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Dynamic Anomaly Detector (Core Model)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Feature Alignment Engine                          â”‚  â”‚
â”‚  â”‚  â€¢ Isolation Forest Model                            â”‚  â”‚
â”‚  â”‚  â€¢ Feature Statistics Tracker                        â”‚  â”‚
â”‚  â”‚  â€¢ Missing Value Handler                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

- **Machine Learning**: scikit-learn (Isolation Forest)
- **API Framework**: FastAPI with async support
- **Data Processing**: pandas, numpy
- **Model Persistence**: joblib
- **Containerization**: Docker & Docker Compose
- **Validation**: Pydantic models

## ğŸ“¦ Installation

### Prerequisites

- Python 3.11 or higher
- pip (Python package manager)
- Docker (optional, for containerized deployment)

### Local Development Setup

1. **Clone the repository**:
```bash
git clone https://github.com/yourusername/dynamic-anomaly-detection.git
cd dynamic-anomaly-detection
```

2. **Create a virtual environment**:
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

4. **Create necessary directories**:
```bash
mkdir -p models data
```

### Docker Setup

```bash
# Build the Docker image
docker build -t dynamic-anomaly-detector .

# Or use Docker Compose
docker-compose up -d
```

## ğŸš€ Quick Start

### 1. Run Example Usage

```bash
python example_usage.py
```

This demonstrates:
- Training with 10 features
- Predicting with same features
- Adapting to new features (5 additional)
- Handling missing features

### 2. Start the API Server

```bash
# Using uvicorn directly
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload

# Or using the Makefile
make run
```

### 3. Test the API

```bash
# Health check
curl http://localhost:8000/health

# Get API documentation
open http://localhost:8000/docs
```

## ğŸ’» Usage Examples

### Python API Usage

#### Basic Model Usage

```python
from src.models.dynamic_anomaly_detector import DynamicAnomalyDetector
import pandas as pd

# Initialize model
model = DynamicAnomalyDetector(contamination=0.1, n_estimators=100)

# Train with initial data (10 features)
train_data = pd.DataFrame({
    'amount': [100.0, 200.0, 150.0, ...],
    'merchant_category': ['retail', 'restaurant', 'grocery', ...],
    'hour_of_day': [14, 19, 10, ...],
    # ... 10 features total
})
model.fit(train_data)

# Predict on new data with same features
predictions, scores = model.predict(test_data, return_scores=True)
anomalies = predictions == -1
print(f"Detected {sum(anomalies)} anomalies")

# Predict on data with NEW features (model adapts automatically!)
new_data = pd.DataFrame({
    'amount': [100.0, 200.0, ...],
    'merchant_category': ['retail', 'restaurant', ...],
    'new_feature_1': [1.5, 2.3, ...],  # New feature!
    'new_feature_2': [0.8, 1.2, ...],  # Another new feature!
    # ... model handles these automatically
})
predictions, scores = model.predict(new_data, return_scores=True)
```

#### REST API Usage

```python
import requests

# Train the model
train_data = [
    {"amount": 100.0, "merchant_category": "retail", "hour_of_day": 14},
    {"amount": 200.0, "merchant_category": "restaurant", "hour_of_day": 19},
    # ... more transactions
]

response = requests.post(
    "http://localhost:8000/train",
    json={"transactions": train_data},
    params={"contamination": 0.1, "n_estimators": 100}
)
print(response.json())

# Single prediction
transaction = {
    "amount": 150.0,
    "merchant_category": "retail",
    "hour_of_day": 14
}

response = requests.post(
    "http://localhost:8000/predict",
    json={"data": transaction}
)
result = response.json()
print(f"Is anomaly: {result['is_anomaly']}")
print(f"Anomaly score: {result['anomaly_score']:.4f}")

# Batch prediction
batch_data = [
    {"amount": 100.0, "merchant_category": "retail", ...},
    {"amount": 200.0, "merchant_category": "restaurant", ...},
]

response = requests.post(
    "http://localhost:8000/predict/batch",
    json={"transactions": batch_data}
)
result = response.json()
print(f"Anomalies detected: {result['anomalies_detected']}/{result['total_transactions']}")
```

### Command Line Usage

```bash
# Train model
curl -X POST "http://localhost:8000/train?contamination=0.1&n_estimators=100" \
  -H "Content-Type: application/json" \
  -d '{
    "transactions": [
      {"amount": 100.0, "merchant_category": "retail", "hour_of_day": 14},
      {"amount": 200.0, "merchant_category": "restaurant", "hour_of_day": 19}
    ]
  }'

# Single prediction
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "data": {
      "amount": 150.0,
      "merchant_category": "retail",
      "hour_of_day": 14
    }
  }'

# Batch prediction
curl -X POST "http://localhost:8000/predict/batch" \
  -H "Content-Type: application/json" \
  -d '{
    "transactions": [
      {"amount": 100.0, "merchant_category": "retail", ...},
      {"amount": 200.0, "merchant_category": "restaurant", ...}
    ]
  }'

# Get model info
curl http://localhost:8000/model/info
```

## ğŸ“š API Documentation

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Root endpoint with API info |
| `GET` | `/health` | Health check endpoint |
| `GET` | `/model/info` | Get model information and statistics |
| `POST` | `/train` | Train the anomaly detection model |
| `POST` | `/predict` | Predict anomaly for single transaction |
| `POST` | `/predict/batch` | Predict anomalies for batch of transactions |
| `POST` | `/update` | Incrementally update model with new data |

### Interactive API Documentation

When the server is running, visit:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Request/Response Examples

#### Train Model

**Request:**
```json
POST /train?contamination=0.1&n_estimators=100
{
  "transactions": [
    {
      "amount": 100.0,
      "merchant_category": "retail",
      "hour_of_day": 14,
      "day_of_week": 1
    }
  ]
}
```

**Response:**
```json
{
  "message": "Model trained successfully",
  "num_samples": 500,
  "num_features": 10,
  "features": ["amount", "merchant_category", "hour_of_day", ...],
  "timestamp": "2024-01-15T10:30:00"
}
```

#### Predict Single Transaction

**Request:**
```json
POST /predict
{
  "data": {
    "amount": 150.0,
    "merchant_category": "retail",
    "hour_of_day": 14
  }
}
```

**Response:**
```json
{
  "is_anomaly": false,
  "anomaly_score": 0.15,
  "prediction": 1,
  "timestamp": "2024-01-15T10:30:00"
}
```

## ğŸ”§ How Dynamic Features Work

### Scenario 1: New Features Added

```
Training Phase:
  Features: [amount, merchant_category, hour_of_day, ...] (10 features)
  Model learns patterns for these 10 features

Inference Phase:
  New data arrives with: [amount, merchant_category, hour_of_day, ..., new_feature_1, new_feature_2] (12 features)
  
  Model automatically:
  1. Detects 2 new features (new_feature_1, new_feature_2)
  2. Adds them to known features list
  3. Initializes statistics for new features
  4. Continues prediction seamlessly
```

### Scenario 2: Missing Features

```
Training Phase:
  Features: [amount, merchant_category, hour_of_day, day_of_week, ...] (10 features)

Inference Phase:
  New data arrives with: [amount, merchant_category, hour_of_day] (3 features, 7 missing)
  
  Model automatically:
  1. Detects missing features (day_of_week, ...)
  2. Uses median/default values from training statistics
  3. Logs warnings for monitoring
  4. Continues prediction normally
```

### Scenario 3: Feature Evolution Over Time

```
Week 1: Train with 10 features
Week 2: New feature added â†’ Model adapts automatically
Week 3: Another feature added â†’ Model adapts automatically
Week 4: Some features deprecated â†’ Model uses defaults

No retraining required!
```

## ğŸš¢ Deployment

### Docker Deployment

```bash
# Build image
docker build -t dynamic-anomaly-detector:latest .

# Run container
docker run -d \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/data:/app/data \
  --name anomaly-detector \
  dynamic-anomaly-detector

# Or use Docker Compose
docker-compose up -d
```

### Docker Compose Configuration

The `docker-compose.yml` includes:
- Health checks
- Volume mounts for model persistence
- Automatic restarts
- Environment variable configuration

### Production Considerations

1. **Environment Variables**:
   ```bash
   export MODEL_PATH=/app/models/anomaly_detector.joblib
   export PYTHONUNBUFFERED=1
   ```

2. **Model Persistence**: Mount volumes for model storage
3. **Monitoring**: Use `/health` endpoint for health checks
4. **Scaling**: Stateless design allows horizontal scaling
5. **Load Balancing**: Use nginx or similar for load distribution

### Kubernetes Deployment (Optional)

The stateless API design makes it Kubernetes-ready. You'll need:
- Deployment manifest
- Service manifest
- ConfigMap for configuration
- PersistentVolume for model storage

## âš¡ Performance

### Benchmarks

- **Inference Speed**: ~1-5ms per transaction
- **Batch Processing**: More efficient for multiple transactions
- **Memory Usage**: ~10-50MB per model (depends on features)
- **Scalability**: Stateless API allows horizontal scaling

### Optimization Tips

1. **Batch Processing**: Use `/predict/batch` for multiple transactions
2. **Feature Selection**: Remove unnecessary features to reduce memory
3. **Model Caching**: Models are automatically cached in memory
4. **Async Processing**: FastAPI handles concurrent requests efficiently

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/test_model.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run example usage
python example_usage.py
```

## ğŸ“ Project Structure

```
dynamic-anomaly-detection/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ dynamic_anomaly_detector.py  # Core ML model
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py                       # FastAPI server
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ data_generator.py            # Sample data generator
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_model.py                    # Unit tests
â”œâ”€â”€ models/                              # Saved models directory
â”œâ”€â”€ data/                                # Data directory
â”œâ”€â”€ Dockerfile                           # Docker configuration
â”œâ”€â”€ docker-compose.yml                   # Docker Compose config
â”œâ”€â”€ requirements.txt                     # Python dependencies
â”œâ”€â”€ Makefile                             # Build automation
â”œâ”€â”€ example_usage.py                     # Usage examples
â”œâ”€â”€ QUICKSTART.md                        # Quick start guide
â”œâ”€â”€ PROJECT_SUMMARY.md                   # Project summary
â””â”€â”€ README.md                            # This file
```

## ğŸ” Configuration

### Model Parameters

- `contamination`: Expected proportion of anomalies (0.0 to 0.5, default: 0.1)
- `n_estimators`: Number of trees in Isolation Forest (default: 100)
- `max_samples`: Number of samples per tree (default: 'auto')
- `scaler_type`: 'robust' or 'standard' (default: 'robust')
- `random_state`: Random seed for reproducibility (default: 42)

### Environment Variables

- `MODEL_PATH`: Path to save/load model (default: `models/anomaly_detector.joblib`)
- `PYTHONUNBUFFERED`: Set to 1 for real-time logging

## ğŸ› Limitations & Future Enhancements

### Current Limitations

1. **Incremental Learning**: Current implementation refits the model. For true incremental learning, consider maintaining a data buffer.
2. **Feature Types**: Currently optimized for numeric features. Categorical features should be encoded.
3. **Model Updates**: When new features are added, the model adapts but doesn't retrain. Consider periodic retraining.

### Planned Enhancements

- [ ] True incremental learning without full retraining
- [ ] Native support for categorical features
- [ ] Model versioning and A/B testing
- [ ] Advanced feature engineering pipeline
- [ ] Monitoring and alerting integration
- [ ] Model explainability (SHAP values)
- [ ] Prometheus metrics integration
- [ ] Structured logging (JSON format)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with [FastAPI](https://fastapi.tiangolo.com/)
- Uses [scikit-learn](https://scikit-learn.org/) for machine learning
- Containerized with [Docker](https://www.docker.com/)

## ğŸ“ Support

For questions, issues, or contributions:
- Open an issue on GitHub
- Check the [Quick Start Guide](QUICKSTART.md)
- Review the [Project Summary](PROJECT_SUMMARY.md)

---

**Made with â¤ï¸ for production-ready anomaly detection**
